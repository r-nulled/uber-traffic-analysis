# Traffic Predictions with Uber Ride Data
Swapnil Lad, Minrui Liang, Dean Long, Sayuj Shajith, Arnold Wang
----------------------------------------------------------------
Final Presentation: https://web.microsoftstream.com/video/6a74b7c9-a52d-4eed-9e1c-a998e7c7c2c0

## Introduction
Many problems of urban transportation begin and end with traffic. Traffic is responsible for increased pollution and increased wait times, and unpredictable traffic can cause complications in personal matters. The ability to accurately predict when traffic will flare up can increase the reliability of trip time estimates, reducing some of the societal friction incurred in-transit by urban and suburban road travelers. 
	Nowadays, a not insignificant percentage of vehicles on the road are vehicles for hire. Companies like Uber pay ordinary vehicle-owners to act as taxis for users of the Uber phone app. Uber and its competitors have an additional incentive to accurately predict trip times: they need to fairly compensate their drivers for their time and fuel. 
 
## Problem Definition
Uber has released data representing a significant portion of the billions of trips booked on their platform from the past few years. Averaged data for all Uber trips departing from a particular origin and arriving at a certain destination over a given time frame can be downloaded in hourly, daily, monthly, and quarterly resolutions. Among the provided statistics is the mean trip time for the origin and destination pair. How can we use past trip data to make consistent and accurate predictions of trip times booked today or tomorrow?
Isolating a single edge (average speed) between two vertices (postal code-resolution locations) simplifies our regression problem into a time-series problem; given historical data, can we predict the average speed for the next time segment?

It’s not guaranteed that historical trip data is enough for us to make a good prediction. However, we can synthesize the Uber dataset with other sources of environmental data to increase our accuracy. Data from Uber is in the form of records of Uber trips within a city during a given time period. An average speed is given for each trip from destination A to B; multiple trips from A to B can be included in the dataset with the same or different average speeds. Apart from date, time, and location data, each average speed is associated with no other features. But because the data provides a date, time, and location, we can add feature data for each trip from other robust data sets; one such data set we have identified and used the “Weather Underground” dataset to provide historical weather data at an equal or greater time resolution than that of the Uber data. Weather data is just an example solution to a more interesting problem; what data can we involve from outside data sources as appended features to each trip datapoint in order to improve our regression?

 
## Methods
### Data Collection
We used Uber Movement trip data to represent general road traffic. Uber provided us with daily and hourly average speeds on road segments in New York over month-long ranges of time. We decided to restrict the Uber data to the Month of February since this was the last month that occurred before lockdown ordinances were imposed in the United States. We then considered a number of potentially correlated geospatial data features to test and include as parameters. 
	We collected weather data for New York in the month of February courtesy of Weather Underground. We used the Python library Beautiful Soup to parse and extract temperature, humidity, and wind speed data representing daily averages for each day in the month of February. We collected daily traffic volume and pedestrian counts which were available on NYC Dot and NYC OpenData respectively. Electricity system demands were also retrieved from the United States Energy Information Administration. These features were joined with the uber trip data using the pandas dataframe using common dates as the matching index. In order to clean the data we dropped entries with missing values, and formatted the date index to be the same across all of the datasets.
Among some of the features that we had considered but did not include were annual average daily traffic and precipitation data. We decided not to include precipitation data due to the lack of rainfall that occurred in New York City in the selected time frame. Most of the values for each of the dates in February had values of zero or had total rainfall of less than a full inch over the course of an entire day, and we decided that the rainfall data would not be significant enough to include in our model. Annual average daily traffic data was not included due to the lack of granularity in this dataset. This dataset provided a daily average traffic value over the course of the year. We were searching instead for daily values for each of the days of the selected time frame. Traffic collision data was considered as well, but was not included due to the lack of sufficient numerical data. Available datasets only provided location of each collision and did not provide any numerical statistics such as daily totals or any frequency data.

### Regression
	In order to build the basis for our study, we decided to first understand how well our parameters would make regressions upon the data of a single location node, considered separately from other locations and adjoining traffic. Given that our trip data consisted of periodically averaged trip speeds over regular periods of time, we decided to conduct our initial analysis for single-edge regressions as time series prediction. Each datapoint consisted of the average speed at which Uber rides passed through a given road segment over the course of a day, tied to the parameter data for the given day of the ride. We used a time series analysis library to perform predictions for future traffic speeds and investigate the predictive power of the parameters we chose to include. The time series analysis tools we used in this part of our initial analysis was Facebook’s open source library Prophet. 
	Once we had understood our approach with the initial testing, we decided to move forward and extend the process to a greater variety of nodes and parameters. We added new regressors that we pruned from the researched datasets (pedestrian count, Electricity Megawatt Calculations, and Total Traffic Volume) and appended these to our original model. We expanded the model to grab a larger portion of the data, and targeted the top 45 with the highest incoming degree. Choosing these allowed for us to run the script over the large portions of data while keeping runtimes relatively manageable. We ordered the data by highest degree as this allowed us to directly study the specific nodes that in turn incurred the highest amount of visits in the dataset. We conducted a time series analysis of these points in order to capture the trends of the dataset as a whole and capture the effectiveness of the model across the entire NYC travel network.
	We separately validated our ideas for correlated parameter data using clustering analysis. After finding that our chosen regressors were worthy of further analysis, we then decided to split the dataset into two parts, one to train the model and the other to test the model. The split was made by cutting the data in half, roughly giving half of the days worth of data in each group. Each day consisted of data collected from thousands of road segments and traffic intersections analyzed in parallel. By taking this approach we were able to provide substantial amounts of data on both ends of the regression. We fed the first half of our data into the Prophet model, with the date and mean travel time being the values used for the linear time series analysis. Temperature, humidity, wind speed, pedestrian count factors, electricity factors, and traffic volume were then added to the model as additional regressors to add complexity to the system. We then moved on to fit the testing data through the FB Prophet library and forecasted to see the results of our model. We studied validation metrics such as mean squared error (MSE) and root mean squared error (RMSE) to help us better understand the validity of our approach.

![alt text](https://github.gatech.edu/awang348/4641-Traffic-Prediction/blob/master/plain_road_graph.PNG)

![alt text](https://github.gatech.edu/awang348/4641-Traffic-Prediction/blob/master/network_with_weights.PNG)

Following our examination of individual nodes, we conducted an analysis of a graph network consisting of 100 nodes representing traffic intersections focused on the node of the largest in-degree. In order to do a simple graph analysis, we considered each edge to have all the remaining edge weights (representing “traffic speed”) as feature parameters and a principal component analysis regression on hourly Uber trip data spanning the month of February. This produced for us 100 perspectives of edge importance. For each edge within the subgraph of consideration, we calculated a measure of importance in predictions. This calculation was an average weighted by each model’s predictive success. 


After finding that our regressors were worthy of further analysis, we then decided to split the month of February (the chosen subset of time that we picked for this initial research) into two parts, one to train the model and the other to test the model. The split was made by cutting the data in half, roughly giving 14 days worth of data in each group. By taking this approach we were able to provide substantial amounts of data on both ends of the regression. We fed the first half of our data into the Prophet model, with the date and mean travel time being the values used for the linear time series analysis. Temperature, humidity, and wind speed were then added to the model as additional regressors to add complexity to the system. We decided to begin our approach with just these three regressors, but we are hoping to take the results from this experiment and further build on our model by introducing additional regressors as we expand to all graph edges. 


We then moved on to fit the testing data through the FB Prophet library and forecasted to see the results of our model. In order to better understand the results of our model, we decided to study some of the validation metrics such as mse and rmse which would help us better understand the validity of our approach.

### Clustering and Feature Selection

We sought to apply clustering as a way of evaluating features within our regression model. We started the clustering by choosing the most common destination, and created a new dataframe that only contains rows that had that place as it’s destination. We originally tried to apply a KMeans clustering algorithm to our data, and we got wildly inaccurate results. We tried some hyperparameter fitting, but we still did not get the results that we wanted. We plotted the clustering and used the visual representation to analyze whether it was a good fit. In the results, it is clear to see that this method was not a good fit for our data.
We then used a more modern algorithm on our data. We used the DBSCAN algorithm and these results ended up making more sense for our data. We could tell that it did a better job of clearly creating clusters in our data than KMeans did. 
We added more features to this data such as pedestrian data that tracked the number of people and direction of travel on the Brooklyn Bridge, energy use and electricity demand, and traffic volume. We weren’t sure which of these would be useful so we tried different combinations of all of them but our results were very disappointing. The clustering methods we used struggled to extract usable information from the extra data that we appended to the dataframe. After doing some investigating we determined that the current structure of the data was insufficient for proper clustering. Since we are using free unlicensed data, the timeframes are very inconsistent. The new features that we tried to incorporate only had one single datapoint for each calendar day, and our previous data-frame had multiple datapoints for every single day. To make this work we had to force each datapoint from our previous dataframe to correlate to the same datapoint in the new dataframe for the correlating date. This hinders any meaningful clustering that can be done and it is shown in the results.

![alt text](https://github.gatech.edu/raw/awang348/4641-Traffic-Prediction/master/final_clustering_1.png?token=AAAGYPEUH4MUQF57RLGCWYS72WK3Y)

![alt text](https://github.gatech.edu/raw/awang348/4641-Traffic-Prediction/master/final_clustering_2.png?token=AAAGYPCI5OA3EXUZ3I5ULTC72WK6Q)

While we were not able to extract useful data with our current setup of our data. We still believe that there is latent information in this data. We believe that more data manipulation and better formatted data is needed to fully realize the latent information these new features contain.

## Results
### Regression
Expanding on the results from the first section, we analyzed 45 nodes with the most number of inbound edges in the first 15 days of february. We chose this as the metric of our selection so that we could further analyze patterns in the nodes that had the most uber visits over the given time period. While we conducted this analysis over a very large pool of data, we chose to highlight a sample of 5 nodes below that represent some of the graphical conclusions that we were able to draw from the dataset:

![alt text](https://github.gatech.edu/awang348/4641-Traffic-Prediction/blob/master/Regression_prediction.png)

The dots on the graph denote the training data points, the line is the actual prediction, and the light blue section color of the graph denotes the confidence interval of our prediction. The wide range of values per day contributed to the wide confidence interval. Even though the model worked to prune extreme outliers, we interpreted these large intervals to mostly be a result of the widely distributed nature of our dataset. The confidence intervals stayed largely similar to the previous iteration of the regression. We ran a similar analysis prior to tackling the larger dataset by taking one node and attempting to extrapolate a similar forecast with only three features (temperature, humidity, and wind speed). We noticed in our final approach that neither increasing the number of parameters nor increasing the granularity and volume of our data helped to reduce the large intervals, further validating our initial hypothesis on the large range of the data.
A sample of the prediction error statistics on the mean speeds are as follows: 

![alt text](https://github.gatech.edu/awang348/4641-Traffic-Prediction/blob/master/Regression_error.png)

As we can see from the tables, the general errors have similar or lower values compared with results from the mid-term report. This could be attributed to the new features added to the regression model. However, note that the results are predicted for the top 45 nodes with in terms of edges meaning that there is sufficient data. The error values might not be repeatable for nodes with less data points. 





### Clustering and Feature Selection

Here are the results from our KMeans clustering (Average Miles Per Hour vs Humidity):

![alt text](https://github.gatech.edu/awang348/4641-Traffic-Prediction/blob/master/4641-midterm-4.PNG)

Here are the results from our DBScan Clustering (Average Miles Per Hour vs Humidity):

![alt text](https://github.gatech.edu/awang348/4641-Traffic-Prediction/blob/master/4641-midterm-5.PNG)

Clustering proved itself as a tool we could use to determine whether an added feature was meaningful for our regression; we could tell that while along some axes the clusters are evident, along others there was no meaningful grouping of attributes. Observing a meaningful split (as we did in this case with normalized average speed along the y-axis) can be interpreted as indicating a meaningful parameter to include for regression. In this case, we can use values from edges leading into and out of a destination vertex. Conversely, there is no split along the humidity axis; this indicates that humidity is a relative non-factor in our regression.


## Discussion and Conclusions
### Summary of Study
Our team intended to bring some insight to traffic analysis using techniques in machine learning, including both supervised and unsupervised learning. We projected to use traffic speed predictions in order to estimate trip times. Producing traffic speed predictions presented a problem more fit for a machine learning solution than the subsequent graph problem of summing edge weights to produce ETA estimates, so we focused on that instead.
While the project did not contribute to the state-of-the-art practices, we did apply our data sources in ways to effectively offer insight in making future traffic speed predictions. In particular, our approach of vetting parameters with multi-dimensional clustering analysis can be applied in future efforts as an indicator of predictive power. For example, performing clustering on our weather parameters allowed us to identify that humidity was a more powerful parameter than temperature, which was more powerful than wind speed. Using this unsupervised analysis, we constructed a supervised learning model that leveraged a variety of environmental and infrastructural features in order to predict transit times along with road conditions. Our predictions were reasonably accurate, achieving RSME values clustered between 0.1 and 0.2, with some as low as 0.07. 

### Future Improvements
Our research stands for future iteration and peer validation;  when looking over the results of our study, there are a few things that come to mind that could be improved. 
	We believe that we should have employed a more methodical approach of evaluating extraneous parameters to include; if we had tested more parameters and chosen the best-predicting candidates our regressions would conceivably be more accurate. We were also very interested in exploring novel architectures to produce regressions on graph edges that could be modelled as time-series on their own; this would have allowed for more complex functions on parameters than our straightforward regressions would allow. 
	It would be interesting to build an additional level of depth to the project that explores the relative paths taken by a vehicle in its transit. For instance, a vehicle that travels from point A to point B may choose between four hypothetical paths, each of which may result in a different prediction. This level of granularity was not available in our dataset, nor is it something that Uber provides due to consumer safety and privacy concerns. There is however, room for Uber’s own team and other researchers with more readily available resources to build upon our studies with this extra level. 
	Another component that we hoped to include in our dataset was the breakdown of socioeconomic classes in the city, and how that might affect the distances that people travel. In line with this, it would have also been interesting to compare the breakdown of projected travel times in different countries against trips of equal distance. Analyzing such parameters would add another qualitative layer to the study that would allow for us to compare how societal factors can cause travel times to increase.
	The last point of improvement we hoped to call out would be to expand the research by increasing the timespan of the data. We hoped to include multiple years worth of data in our analysis, but the computing time required to construct the forecasting model over such an interval began to grow exponentially, so we chose to tackle a smaller subset to analyze and draw our conclusions from. With access to more computing power our model could be refined with a longer timespan, ultimately leading to better prediction results. 

### Applications of Study
There are many practical applications in which our study could be applied or expanded. The first, and most simple, is the ability for Uber and other transportation oriented organizations to take the models that we’ve constructed in NYC and run a similar study in other cities around the country. By keeping the general infrastructure and traffic laws common (as seen within the United States), researchers can better understand and predict the transit times in their area, and work towards understanding which parameters cause the greatest impact in their region. In addition, there is room for our study to be used in underdeveloped countries as well. City planners and legislators who are working to design the infrastructure and roadways for a new city or renovations can use the characteristics of their area in choosing how to design their roadways and how to plan the layout of certain buildings to ease the transit of their residents. In general, we hope that a wide variety of researchers are able to take the analysis that we have done in this one city over a limited time frame, and extrapolate it over a variety of locations and times to improve the conditions of their company, products, or citizens. 
	Another critical place that this tool could be used is for product delivery, especially food delivery places, such as family-owned restaurants and local businesses, such as pizza parlors that offer delivery within a small radius. Planners can utilize this study to understand and predict delivery times for their drivers to reach their destinations based on the environmental and geographical characteristics of the area that they serve. For instance, a small pizza shop can study how long it takes for its small group of drivers to deliver pizza over a few months period, and then use our study to build a predictive analysis that can allow for them to forecast and guarantee delivery times to their customer base. The applications of this process can be applied to a wide variety of small organizations such as the pizza shop, allowing for them to compete with larger chains and work towards increasing customer satisfaction and awareness. 
	
